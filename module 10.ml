{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNnztKcqH3tTZjMPcPfngw3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Question 1: What is Boosting in Machine Learning? Explain how it improves weak\n","learners.\n","\n","Boosting is a powerful ensemble learning technique in machine learning that aims to improve the accuracy of predictive models by combining multiple weak learners to form a strong learner. A weak learner is a model that performs only slightly better than random guessing, such as a simple decision stump (a decision tree with one split).\n","\n","What is Boosting?\n","\n","Boosting constructs a strong predictive model by sequentially training a series of weak learners, where each learner tries to correct the errors of its predecessor. Unlike methods like bagging, where models are trained independently and in parallel, boosting trains learners one after another. This sequential process allows the model to focus on the most difficult examples that previous models struggled with.\n","\n","How Boosting Works\n","\n","Initialization:\n","Initially, all training examples are given equal weight.\n","\n","Training Weak Learners:\n","A weak learner is trained on the weighted training data. After the first learner, the algorithm evaluates the errors made.\n","\n","Updating Weights:\n","The weights of misclassified samples are increased so that the next learner pays more attention to these harder examples. Correctly classified samples may have their weights decreased.\n","\n","Combining Learners:\n","Each learner is assigned a weight based on its accuracy. Predictions from all weak learners are combined through a weighted vote (classification) or weighted sum (regression) to make the final prediction.\n","\n","How Boosting Improves Weak Learners\n","\n","Focus on Hard Examples:\n","By increasing the weight of misclassified samples, boosting forces the subsequent weak learners to concentrate on difficult cases that previous models missed, improving overall learning.\n","\n","Error Reduction:\n","Since each learner tries to correct the mistakes of the previous learners, the combined model gradually reduces both bias and variance.\n","\n","Weighted Combination:\n","By weighting the learners according to their accuracy, boosting ensures that more reliable models have a bigger influence on the final prediction.\n","\n","Popular Boosting Algorithms\n","\n","AdaBoost (Adaptive Boosting):\n","The original boosting algorithm that adjusts weights of training samples and learners adaptively based on error.\n","\n","Gradient Boosting:\n","Boosting is framed as an optimization problem, where each new learner fits the residual errors (gradients) of the combined previous learners.\n","\n","XGBoost:\n","An efficient and scalable implementation of gradient boosting with regularization to reduce overfitting.\n","\n","Summary\n","\n","Boosting effectively converts a set of weak learners into a strong learner by sequentially training models to focus on the errors of prior learners and combining them in a weighted manner. This approach significantly improves predictive accuracy and is widely used in both classification and regression problems.\n","\n"],"metadata":{"id":"oY2yuEm69qZo"}},{"cell_type":"markdown","source":["Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n","\n","\n","AdaBoost and Gradient Boosting are both popular boosting algorithms used to improve the performance of weak learners by combining them sequentially. Although they share the general boosting framework, their training processes differ significantly in how they update the models and handle errors.\n","\n","AdaBoost (Adaptive Boosting)\n","\n","Training Process:\n","AdaBoost trains weak learners sequentially by adjusting the weights of training examples after each iteration. Initially, all data points have equal weights. After each weak learner is trained, AdaBoost increases the weights of misclassified samples and decreases the weights of correctly classified samples. This reweighting forces the next weak learner to focus more on the difficult cases that the previous model failed to classify correctly.\n","\n","Model Update:\n","Each weak learner is assigned a weight based on its accuracy (often related to the error rate). The final prediction is a weighted majority vote of all weak learners, where more accurate learners have higher influence.\n","\n","Error Correction:\n","AdaBoost explicitly adjusts the importance of individual training samples to focus on harder cases by modifying their weights.\n","\n","Gradient Boosting\n","\n","Training Process:\n","Gradient Boosting views boosting as a gradient descent optimization problem in function space. Instead of reweighting samples, it sequentially fits new weak learners to the residual errors (the difference between the observed values and the current model predictions) made by the combined previous models.\n","\n","Model Update:\n","Each new learner is trained to predict the negative gradient of the loss function with respect to the current model’s output. This means it tries to minimize a specified loss function (e.g., mean squared error for regression, log-loss for classification) by fitting the residuals.\n","\n","Error Correction:\n","Gradient Boosting corrects errors by directly modeling the residuals (gradients), which provides a more flexible and generalized way to improve the model iteratively.\n","\n","Summary\n","\n","In essence, AdaBoost improves weak learners by adaptively reweighting training samples to focus on hard-to-classify points, while Gradient Boosting improves models by fitting new learners to the residual errors of previous models, using gradient descent to minimize a chosen loss function. Gradient Boosting is more general and flexible, making it applicable to a broader range of problems.\n","\n"],"metadata":{"id":"wxiYMlTK94xE"}},{"cell_type":"markdown","source":["Question 3: How does regularization help in XGBoost?\n","\n","Regularization is a crucial technique in machine learning that helps prevent overfitting, which occurs when a model learns the noise in the training data instead of the underlying pattern. XGBoost (Extreme Gradient Boosting), a highly efficient and scalable implementation of gradient boosting, incorporates regularization explicitly to improve model generalization and performance.\n","\n","What is Regularization in XGBoost?\n","\n","In XGBoost, regularization adds a penalty term to the objective function, which consists of the loss function plus the regularization term. The objective function that XGBoost minimizes can be written as:\n","\n","Objective\n","=\n","Loss Function\n","+\n","Ω\n","(\n","Model\n",")\n","Objective=Loss Function+Ω(Model)\n","\n","where\n","Ω\n","(\n","Model\n",")\n","Ω(Model) is the regularization term that penalizes the complexity of the model.\n","\n","Types of Regularization in XGBoost\n","\n","L1 Regularization (Lasso):\n","This adds the sum of the absolute values of the leaf weights as a penalty. L1 regularization encourages sparsity in the model by driving some leaf weights to zero, effectively performing feature selection and simplifying the model.\n","\n","L2 Regularization (Ridge):\n","This adds the sum of the squared leaf weights as a penalty. L2 regularization discourages large weights, promoting smaller, smoother predictions that reduce overfitting.\n","\n","Tree Complexity Penalty:\n","XGBoost also penalizes the number of leaves in the trees to control model complexity. This prevents trees from growing too deep and becoming overly complex.\n","\n","How Regularization Helps in XGBoost\n","\n","Controls Overfitting:\n","By penalizing complex models (with many leaves or large leaf weights), regularization limits the model's capacity to memorize noise from the training data. This improves the model's ability to generalize well on unseen data.\n","\n","Encourages Simpler Models:\n","Regularization pushes the model to prefer simpler trees with fewer leaves and smaller weights, which are easier to interpret and less prone to capturing noise.\n","\n","Improves Model Stability:\n","It prevents large fluctuations in predictions caused by extreme values in leaf weights, leading to more stable and reliable models.\n","\n","Balances Bias-Variance Trade-off:\n","Regularization helps find the right balance between bias (underfitting) and variance (overfitting), which is critical for optimal predictive performance.\n","\n","Summary\n","\n","Regularization in XGBoost, through L1 and L2 penalties on leaf weights and tree complexity control, plays a vital role in reducing overfitting and improving the model’s generalization. It helps ensure that the ensemble of trees is not overly complex, thereby enhancing the robustness and predictive accuracy of the model on new, unseen data.v"],"metadata":{"id":"NvGwtVEK-HHx"}},{"cell_type":"markdown","source":["Question 3: How does regularization help in XGBoost?\n","\n","CatBoost (Categorical Boosting) is a gradient boosting algorithm specifically designed to handle categorical features efficiently and effectively. Unlike many traditional machine learning algorithms that require extensive preprocessing of categorical data, CatBoost incorporates specialized techniques that allow it to process categorical variables natively, improving both model performance and ease of use.\n","\n","Challenges with Categorical Data\n","\n","Categorical data represents features with discrete values or categories (e.g., color, country, product type). Handling these variables in machine learning can be challenging because most algorithms require numerical input. Common preprocessing techniques include:\n","\n","One-Hot Encoding: Converts each category into binary vectors, which can lead to high dimensionality and sparsity.\n","\n","Label Encoding: Converts categories to integer codes, but may introduce unintended ordinal relationships.\n","\n","Target Encoding: Replaces categories with target statistics, but risks data leakage if not done carefully.\n","\n","How CatBoost Efficiently Handles Categorical Data\n","\n","Native Support for Categorical Features:\n","CatBoost accepts categorical features as input directly, without requiring manual encoding. This simplifies the data preprocessing pipeline.\n","\n","Ordered Target Statistics:\n","CatBoost uses a novel method called ordered target encoding, which computes statistics (like mean target value) for each category but does so in an ordered fashion that avoids target leakage. It uses permutations of the training data to ensure that when calculating the encoding for a data point, information from future data points is not used.\n","\n","Efficient Handling of High Cardinality:\n","CatBoost’s encoding approach handles categorical features with many categories (high cardinality) effectively without blowing up the feature space, unlike one-hot encoding.\n","\n","Robustness to Overfitting:\n","The ordered target statistics reduce the risk of overfitting compared to traditional target encoding, because they prevent leakage of target information into the features.\n","\n","Combination of Features:\n","CatBoost can create and learn from combinations of categorical features internally, enhancing the model’s ability to capture complex patterns.\n","\n","Advantages of CatBoost for Categorical Data\n","\n","Reduced Preprocessing Effort:\n","No need for manual encoding or extensive feature engineering on categorical variables.\n","\n","Improved Accuracy:\n","The specialized encoding techniques lead to better generalization and more accurate models.\n","\n","Faster Training:\n","Efficient handling of categorical data reduces training time compared to other methods requiring large encoded feature sets.\n","\n","Summary\n","\n","CatBoost is considered efficient for handling categorical data because it natively processes categorical features using innovative ordered target statistics, avoiding common pitfalls like target leakage and high-dimensional sparse encoding. This leads to simpler pipelines, reduced overfitting, and improved model performance, especially on datasets with many and high-cardinality categorical features."],"metadata":{"id":"_BkAwscg-UBs"}},{"cell_type":"markdown","source":["Question 4: Why is CatBoost considered efficient for handling categorical data?\n","\n","Answer:\n","\n","CatBoost (Categorical Boosting) is a gradient boosting algorithm specifically designed to handle categorical features efficiently and effectively. Unlike many traditional machine learning algorithms that require extensive preprocessing of categorical data, CatBoost incorporates specialized techniques that allow it to process categorical variables natively, improving both model performance and ease of use.\n","\n","Challenges with Categorical Data\n","\n","Categorical data represents features with discrete values or categories (e.g., color, country, product type). Handling these variables in machine learning can be challenging because most algorithms require numerical input. Common preprocessing techniques include:\n","\n","One-Hot Encoding: Converts each category into binary vectors, which can lead to high dimensionality and sparsity.\n","\n","Label Encoding: Converts categories to integer codes, but may introduce unintended ordinal relationships.\n","\n","Target Encoding: Replaces categories with target statistics, but risks data leakage if not done carefully.\n","\n","How CatBoost Efficiently Handles Categorical Data\n","\n","Native Support for Categorical Features:\n","CatBoost accepts categorical features as input directly, without requiring manual encoding. This simplifies the data preprocessing pipeline.\n","\n","Ordered Target Statistics:\n","CatBoost uses a novel method called ordered target encoding, which computes statistics (like mean target value) for each category but does so in an ordered fashion that avoids target leakage. It uses permutations of the training data to ensure that when calculating the encoding for a data point, information from future data points is not used.\n","\n","Efficient Handling of High Cardinality:\n","CatBoost’s encoding approach handles categorical features with many categories (high cardinality) effectively without blowing up the feature space, unlike one-hot encoding.\n","\n","Robustness to Overfitting:\n","The ordered target statistics reduce the risk of overfitting compared to traditional target encoding, because they prevent leakage of target information into the features.\n","\n","Combination of Features:\n","CatBoost can create and learn from combinations of categorical features internally, enhancing the model’s ability to capture complex patterns.\n","\n","Advantages of CatBoost for Categorical Data\n","\n","Reduced Preprocessing Effort:\n","No need for manual encoding or extensive feature engineering on categorical variables.\n","\n","Improved Accuracy:\n","The specialized encoding techniques lead to better generalization and more accurate models.\n","\n","Faster Training:\n","Efficient handling of categorical data reduces training time compared to other methods requiring large encoded feature sets.\n","\n","Summary\n","\n","CatBoost is considered efficient for handling categorical data because it natively processes categorical features using innovative ordered target statistics, avoiding common pitfalls like target leakage and high-dimensional sparse encoding. This leads to simpler pipelines, reduced overfitting, and improved model performance, especially on datasets with many and high-cardinality categorical features."],"metadata":{"id":"vrXwAgvd-sJX"}},{"cell_type":"markdown","source":["Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n","\n","Answer:\n","\n","Boosting and bagging are both popular ensemble learning methods, but they have different strengths and are preferred in different real-world scenarios based on their characteristics.\n","\n","Key Differences Recap\n","\n","Bagging (Bootstrap Aggregating):\n","Builds multiple independent models (usually decision trees) in parallel on different random samples of data, then combines them by averaging or voting. It reduces variance and is effective in stabilizing high-variance models like deep decision trees.\n","\n","Boosting:\n","Builds models sequentially, where each new model focuses on correcting errors from the previous ones. It reduces both bias and variance and often achieves higher accuracy but can be more prone to overfitting without proper regularization.\n","\n","Real-World Applications Where Boosting is Preferred\n","\n","Credit Scoring and Fraud Detection\n","\n","Boosting algorithms like XGBoost and LightGBM are widely used in the financial sector for credit risk modeling and fraud detection.\n","\n","These problems require highly accurate and robust models that can detect subtle patterns and rare events.\n","\n","Boosting’s focus on difficult-to-classify cases helps improve sensitivity to fraudulent transactions or high-risk borrowers.\n","\n","Customer Churn Prediction\n","\n","Companies in telecom, subscription services, and SaaS industries use boosting models to predict customer churn.\n","\n","Boosting models handle imbalanced datasets well by focusing on customers who are likely to leave, improving retention strategies.\n","\n","Search Ranking and Recommendation Systems\n","\n","Boosting techniques are used in ranking algorithms for search engines (e.g., Bing, Yandex) and recommendation systems.\n","\n","They can model complex interactions between features, improving the relevance of search results and personalized recommendations.\n","\n","Healthcare and Medical Diagnosis\n","\n","Boosting is applied in predictive models for disease diagnosis, patient risk stratification, and treatment outcome prediction.\n","\n","The ability to reduce bias and improve accuracy is critical when decisions impact patient health.\n","\n","Image and Text Classification\n","\n","Boosting algorithms are often used as strong baselines or components in computer vision and natural language processing tasks.\n","\n","Their ability to focus on hard-to-classify examples improves performance on nuanced datasets.\n","\n","Sales Forecasting and Demand Prediction\n","\n","Retail and supply chain industries leverage boosting models to predict product demand and optimize inventory.\n","\n","The sequential learning approach helps capture complex temporal and categorical relationships in sales data.\n","\n","Why Boosting is Preferred Over Bagging in These Applications\n","\n","Higher Predictive Accuracy:\n","Boosting often achieves better accuracy by reducing both bias and variance, essential for high-stakes or competitive applications.\n","\n","Handling Imbalanced Data:\n","Boosting’s iterative focus on misclassified samples makes it better suited for datasets with rare classes or skewed distributions.\n","\n","Capturing Complex Patterns:\n","Boosting models can capture intricate feature interactions and nonlinear relationships more effectively than bagging.\n","\n","Flexibility with Loss Functions:\n","Gradient boosting allows optimization of customized loss functions tailored to specific business objectives.\n","\n","Summary\n","\n","Boosting techniques are preferred in real-world applications where achieving the highest predictive accuracy and handling complex, imbalanced data are critical. These include financial risk modeling, fraud detection, churn prediction, search ranking, healthcare diagnostics, and demand forecasting. Their sequential learning process and error-focused approach provide advantages over bagging methods in these domains."],"metadata":{"id":"ITYkw50q-xsC"}},{"cell_type":"markdown","source":["t Question 6: Write a Python program to: ● Train an AdaBoost Classifier on the Breast Cancer dataset ● Print the model accuracy?"],"metadata":{"id":"0s6Q_5Qs-3tH"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load Breast Cancer dataset\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","# Split dataset into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize AdaBoost classifier\n","ada = AdaBoostClassifier(random_state=42)\n","\n","# Train the model\n","ada.fit(X_train, y_train)\n","\n","# Make predictions on test set\n","y_pred = ada.predict(X_test)\n","\n","# Calculate and print accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"AdaBoost Classifier Accuracy on Breast Cancer dataset: {accuracy:.4f}\")\n"],"metadata":{"id":"NY9cruCS_Bsg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AdaBoost Classifier Accuracy on Breast Cancer dataset: 0.9649\n","\n"],"metadata":{"id":"YHZ5WkEV_Nmt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question 7: Write a Python program to:\n","● Train a Gradient Boosting Regressor on the California Housing dataset\n","● Evaluate performance using R-squared score"],"metadata":{"id":"JBZrPa5Y_Rhe"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.metrics import r2_score\n","\n","# Load California Housing dataset\n","data = fetch_california_housing()\n","X = data.data\n","y = data.target\n","\n","# Split dataset into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize Gradient Boosting Regressor\n","gbr = GradientBoostingRegressor(random_state=42)\n","\n","# Train the model\n","gbr.fit(X_train, y_train)\n","\n","# Make predictions on test set\n","y_pred = gbr.predict(X_test)\n","\n","# Calculate and print R-squared score\n","r2 = r2_score(y_test, y_pred)\n","print(f\"Gradient Boosting Regressor R-squared score on California Housing dataset: {r2:.4f}\")\n"],"metadata":{"id":"Mc8FpOVC_W3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Gradient Boosting Regressor R-squared score on California Housing dataset: 0.8056\n"],"metadata":{"id":"vnVJIeSP_acM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question 8: Write a Python program to:\n","● Train an XGBoost Classifier on the Breast Cancer dataset\n","● Tune the learning rate using GridSearchCV\n","● Print the best parameters and accuracy\n"],"metadata":{"id":"LQUvx7ko_fE5"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from xgboost import XGBClassifier\n","\n","# Load Breast Cancer dataset\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","# Split dataset into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize XGBoost Classifier\n","xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n","\n","# Define parameter grid for learning rate tuning\n","param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]}\n","\n","# Setup GridSearchCV\n","grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n","\n","# Train models and search best learning rate\n","grid_search.fit(X_train, y_train)\n","\n","# Get best estimator and predict on test set\n","best_model = grid_search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print best parameters and accuracy\n","print(\"Best learning rate:\", grid_search.best_params_['learning_rate'])\n","print(f\"Accuracy with best learning rate: {accuracy:.4f}\")\n"],"metadata":{"id":"feJxctjE_jM8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question 9: Write a Python program to:\n","● Train a CatBoost Classifier\n","● Plot the confusion matrix using seaborn"],"metadata":{"id":"IXLR9Z5B_o39"}},{"cell_type":"code","source":["# Import necessary libraries\n","from catboost import CatBoostClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load Breast Cancer dataset\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","# Split dataset into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize CatBoost Classifier\n","model = CatBoostClassifier(verbose=0, random_state=42)\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Compute confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Plot confusion matrix using seaborn heatmap\n","plt.figure(figsize=(6,5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Confusion Matrix - CatBoost Classifier')\n","plt.show()\n"],"metadata":{"id":"YxrYVJNP_1wy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question 10: You're working for a FinTech company trying to predict loan default using\n","customer demographics and transaction behavior.\n","The dataset is imbalanced, contains missing values, and has both numeric and\n","categorical features.\n","Describe your step-by-step data science pipeline using boosting techniques:\n","● Data preprocessing & handling missing/categorical values\n","● Choice between AdaBoost, XGBoost, or CatBoost\n","● Hyperparameter tuning strategy\n","● Evaluation metrics you'd choose and why\n","● How the business would benefit from your model"],"metadata":{"id":"5fe6S3Pt_6xu"}},{"cell_type":"markdown","source":["1. Data Preprocessing & Handling Missing/Categorical Values\n","\n","Missing Values:\n","\n","Analyze the missing data pattern.\n","\n","For numerical features, consider imputing missing values using median or mean imputation or model-based imputation like KNN imputer.\n","\n","For categorical features, impute with the most frequent category or a new category like 'Missing'.\n","\n","Alternatively, use boosting algorithms (like CatBoost or XGBoost) that can handle missing values natively.\n","\n","Categorical Features:\n","\n","Since the dataset has categorical variables, use algorithms that handle categorical data efficiently.\n","\n","CatBoost natively supports categorical features by applying ordered target statistics without explicit encoding.\n","\n","For AdaBoost or XGBoost, categorical variables typically need encoding:\n","\n","Use one-hot encoding for low cardinality features.\n","\n","Use target or frequency encoding for high cardinality features with caution to avoid leakage.\n","\n","Data Imbalance:\n","\n","Loan default datasets are often imbalanced (few defaults).\n","\n","Use techniques such as:\n","\n","Resampling: SMOTE (Synthetic Minority Over-sampling Technique), ADASYN, or random under/oversampling.\n","\n","Algorithmic approaches: Use boosting models that support class weighting or scale_pos_weight (XGBoost) to handle imbalance.\n","\n","Use evaluation metrics that reflect imbalance (discussed below).\n","\n","Feature Engineering:\n","\n","Create new features from transaction history (e.g., average transaction amount, transaction frequency).\n","\n","Encode date/time variables if present.\n","\n","Use domain knowledge to generate meaningful features.\n","\n","2. Choice Between AdaBoost, XGBoost, or CatBoost\n","\n","CatBoost is highly recommended:\n","\n","Handles categorical features natively.\n","\n","Robust to missing values without extensive preprocessing.\n","\n","Efficient on imbalanced datasets by using built-in class weights.\n","\n","Generally requires less manual tuning and preprocessing than AdaBoost or XGBoost.\n","\n","XGBoost is a strong alternative if careful preprocessing and encoding are done.\n","\n","AdaBoost is less flexible with categorical and imbalanced data but can work if features are properly prepared.\n","\n","3. Hyperparameter Tuning Strategy\n","\n","Use GridSearchCV or RandomizedSearchCV for systematic hyperparameter tuning.\n","\n","Important hyperparameters to tune:\n","\n","Learning rate: Controls step size at each iteration.\n","\n","Number of estimators: Number of boosting rounds.\n","\n","Max depth / tree complexity: Controls model complexity.\n","\n","Subsample ratio: Helps prevent overfitting.\n","\n","Colsample_bytree (XGBoost/CatBoost): Fraction of features used per tree.\n","\n","Class weights / scale_pos_weight: For imbalanced datasets.\n","\n","For CatBoost, also tune parameters like:\n","\n","border_count (for categorical splits)\n","\n","l2_leaf_reg (L2 regularization)\n","\n","Consider using Bayesian Optimization or Hyperopt for more efficient tuning if resources allow.\n","\n","4. Evaluation Metrics & Why\n","\n","Area Under the ROC Curve (AUC-ROC):\n","\n","Measures the model’s ability to distinguish between classes across all classification thresholds.\n","\n","Good for imbalanced classification as it is insensitive to class distribution.\n","\n","Precision, Recall, and F1-Score:\n","\n","Focus on Recall (sensitivity) to minimize false negatives (predicting non-default when default actually occurs).\n","\n","F1-score balances precision and recall.\n","\n","Confusion Matrix:\n","\n","To understand type I and type II errors explicitly.\n","\n","Precision-Recall Curve & Average Precision:\n","\n","More informative than ROC when dealing with highly imbalanced data.\n","\n","Business Cost Metrics:\n","\n","If available, incorporate cost-sensitive metrics like expected loss from false positives/negatives.\n","\n","5. Business Benefits from Your Model\n","\n","Improved Risk Assessment:\n","\n","Early identification of likely defaulters allows proactive risk mitigation (e.g., adjusting loan terms, targeted customer communication).\n","\n","Reduced Financial Losses:\n","\n","Minimizing defaults directly reduces financial losses and improves profitability.\n","\n","Better Customer Segmentation:\n","\n","Identifying reliable customers enables offering personalized services or incentives.\n","\n","Regulatory Compliance:\n","\n","Transparent and robust models support compliance with lending regulations and fair lending practices.\n","\n","Operational Efficiency:\n","\n","Automating loan default prediction streamlines decision-making and reduces manual effort.\n","\n","Competitive Advantage:\n","\n","Leveraging advanced boosting techniques enhances predictive accuracy, positioning the company ahead in market intelligence."],"metadata":{"id":"OrG4lgdlAJb3"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from catboost import CatBoostClassifier, Pool\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, roc_auc_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Synthetic dataset creation for demo purposes\n","np.random.seed(42)\n","n_samples = 1000\n","\n","# Numeric features with some missing values\n","num_feat = pd.DataFrame({\n","    'age': np.random.randint(18, 70, n_samples),\n","    'income': np.random.normal(50000, 15000, n_samples),\n","    'transaction_count': np.random.poisson(10, n_samples)\n","})\n","# Introduce missing values randomly\n","for col in num_feat.columns:\n","    num_feat.loc[num_feat.sample(frac=0.1).index, col] = np.nan\n","\n","# Categorical features\n","cat_feat = pd.DataFrame({\n","    'gender': np.random.choice(['Male', 'Female'], n_samples),\n","    'education': np.random.choice(['High School', 'Bachelors', 'Masters', 'PhD'], n_samples),\n","    'marital_status': np.random.choice(['Single', 'Married', 'Divorced'], n_samples)\n","})\n","\n","# Target variable (imbalanced)\n","target = np.random.choice([0, 1], size=n_samples, p=[0.85, 0.15])  # 15% defaults\n","\n","# Combine all features into one DataFrame\n","df = pd.concat([num_feat, cat_feat], axis=1)\n","df['default'] = target\n","\n","# Handle missing numeric values by simple median imputation\n","imputer = SimpleImputer(strategy='median')\n","df[num_feat.columns] = imputer.fit_transform(df[num_feat.columns])\n","\n","# Encode categorical features using LabelEncoder (CatBoost can handle raw categorical, but sklearn needs numeric)\n","for col in cat_feat.columns:\n","    df[col] = LabelEncoder().fit_transform(df[col])\n","\n","# Separate features and target\n","X = df.drop('default', axis=1)\n","y = df['default']\n","\n","# Split into train-test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Identify categorical feature indices for CatBoost\n","cat_features_indices = [X.columns.get_loc(col) for col in cat_feat.columns]\n","\n","# Create Pool objects for CatBoost (helps handle categorical features & missing values)\n","train_pool = Pool(data=X_train, label=y_train, cat_features=cat_features_indices)\n","test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features_indices)\n","\n","# Initialize CatBoostClassifier with class_weights to handle imbalance\n","model = CatBoostClassifier(\n","    iterations=500,\n","    learning_rate=0.05,\n","    depth=6,\n","    eval_metric='AUC',\n","    random_seed=42,\n","    early_stopping_rounds=30,\n","    class_weights=[1, 6],  # weight default class higher because it's minority\n","    verbose=100\n",")\n","\n","# Train the model\n","model.fit(train_pool, eval_set=test_pool)\n","\n","# Predict probabilities and classes\n","y_pred_proba = model.predict_proba(X_test)[:, 1]\n","y_pred = model.predict(X_test)\n","\n","# Evaluate model\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n"],"metadata":{"id":"g_ChigrUAV1X"},"execution_count":null,"outputs":[]}]}